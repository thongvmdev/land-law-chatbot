import fitz  # PyMuPDF
import re
import json
from typing import List, Dict, Any


def load_and_concatenate_external_json(
    json_file_path: str, existing_chunks: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Load and concatenate content from an external JSON file to existing chunks.

    Args:
        json_file_path: Path to the external JSON file to load
        existing_chunks: List of existing chunks to extend

    Returns:
        Extended list of chunks with external data appended
    """
    try:
        with open(json_file_path, "r", encoding="utf-8") as f:
            external_data = json.load(f)

        if isinstance(external_data, list):
            existing_chunks.extend(external_data)
            print(f"â• ÄÃ£ thÃªm {len(external_data)} chunks tá»« {json_file_path}")
            print(f"ğŸ“Š Tá»•ng cá»™ng sau khi ghÃ©p: {len(existing_chunks)} chunks")
        else:
            print(f"âš ï¸ Cáº£nh bÃ¡o: {json_file_path} khÃ´ng pháº£i lÃ  array, bá» qua viá»‡c ghÃ©p")

    except FileNotFoundError:
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file {json_file_path}, tiáº¿p tá»¥c vá»›i dá»¯ liá»‡u hiá»‡n táº¡i")
    except json.JSONDecodeError as e:
        print(f"âŒ Lá»—i Ä‘á»c JSON tá»« {json_file_path}: {e}")

    return existing_chunks


class LandLawChunkerFinal:
    def __init__(self, pdf_path, max_pages=None):
        self.pdf_path = pdf_path
        self.max_pages = max_pages
        try:
            self.doc = fitz.open(pdf_path)
        except Exception as e:
            raise ValueError(f"KhÃ´ng thá»ƒ má»Ÿ file PDF: {e}")

        self.law_id = "133/VBHN-VPQH"
        self.chunks = []

        # State variables (Hierarchy)
        self.current_chapter = {"id": None, "title": None}
        self.current_section = {"id": None, "title": None}

        # [Má»šI] Kho lÆ°u trá»¯ Footnote theo trang: { page_num: "ná»™i dung footnote" }
        self.page_footnotes_map = {}

        # [Má»šI] Báº£n Ä‘á»“ Ã¡nh xáº¡ tá»« Index trong full_text sang Sá»‘ trang
        # Format: [{"page": 1, "start": 0, "end": 1000}, ...]
        self.page_offset_map = []

    def get_page_content_and_footnotes(self, page):
        """
        Tráº£ vá» 2 giÃ¡ trá»‹:
        1. clean_text: Ná»™i dung chÃ­nh (cá»¡ chá»¯ to)
        2. footnote_text: Ná»™i dung chÃº thÃ­ch (cá»¡ chá»¯ nhá»)
        """

        blocks = page.get_text(
            "dict",
            flags=fitz.TEXT_PRESERVE_LIGATURES | fitz.TEXT_PRESERVE_WHITESPACE,
        )["blocks"]
        clean_text = ""
        footnote_text = ""

        # NgÆ°á»¡ng cá»¡ chá»¯ phÃ¢n loáº¡i
        FONT_SIZE_THRESHOLD = 12

        for b in blocks:
            if "lines" in b:
                for l in b["lines"]:
                    line_clean = ""
                    line_note = ""
                    for s in l["spans"]:
                        text_segment = s["text"]
                        if s["size"] > FONT_SIZE_THRESHOLD:
                            line_clean += text_segment
                        else:
                            # Lá»c rÃ¡c: Bá» qua sá»‘ trang Ä‘Æ¡n láº» náº¿u nÃ³ láº«n vÃ o footnote
                            if not re.match(r"^\s*\d+\s*$", text_segment):
                                line_note += text_segment

                    if line_clean.strip():
                        clean_text += line_clean + "\n"
                    if line_note.strip():
                        footnote_text += line_note + " "  # Ná»‘i footnote thÃ nh dÃ²ng dÃ i

        return clean_text, footnote_text.strip()

    def log_structure_hierarchy(self, matches):
        """
        In ra cáº¥u trÃºc cÃ¢y cá»§a vÄƒn báº£n luáº­t dá»±a trÃªn káº¿t quáº£ Regex.
        """
        print(f"\nğŸ” TÃ¬m tháº¥y {len(matches)} Ä‘iá»ƒm Ä‘Ã¡nh dáº¥u cáº¥u trÃºc.")
        print("=" * 60)
        print(f"{'LOáº I':<10} | {'CHI TIáº¾T':<50}")
        print("=" * 60)

        count_chuong = 0
        count_muc = 0
        count_dieu = 0

        for m in matches:
            marker = m.group(1).strip()  # VD: ChÆ°Æ¡ng I, Má»¥c 1, Äiá»u 1.
            title = m.group(3).strip()  # VD: Pháº¡m vi Ä‘iá»u chá»‰nh

            if marker.startswith("ChÆ°Æ¡ng"):
                count_chuong += 1
                print(f"ğŸ“˜ {marker}: {title.upper()}")
            elif marker.startswith("Má»¥c"):
                count_muc += 1
                print(f"  ğŸ“‚ {marker}: {title}")
            elif marker.startswith("Äiá»u"):
                count_dieu += 1
                display_title = (title[:50] + "...") if len(title) > 50 else title
                print(f"    ğŸ“„ {marker} {display_title}")

        print("=" * 60)
        print(
            f"ğŸ“Š THá»NG KÃŠ: {count_chuong} ChÆ°Æ¡ng | {count_muc} Má»¥c | {count_dieu} Äiá»u"
        )
        print("=" * 60 + "\n")

    def clean_text_for_embedding(self, text):
        """
        LÃ m sáº¡ch text triá»‡t Ä‘á»ƒ Ä‘á»ƒ lÆ°u vÃ o DB (dÃ¹ng cho semantic search).
        """
        # 1. XÃ³a Ä‘Ã¡nh dáº¥u trang vÃ  header/footer cá»‘ Ä‘á»‹nh
        # XÃ³a dÃ²ng "--- PAGE 123 ---"
        text = re.sub(r"--- PAGE \d+ ---", "", text)
        # XÃ³a cÃ¡c sá»‘ trang Ä‘Æ¡n láº» á»Ÿ Ä‘áº§u/cuá»‘i dÃ²ng (thÆ°á»ng lÃ  sá»‘ trang)
        text = re.sub(r"^\s*\d+\s*$", "", text, flags=re.MULTILINE)
        # XÃ³a sá»‘ trang á»Ÿ giá»¯a dÃ²ng vá»›i format " - 123 - " hoáº·c " 123 "
        text = re.sub(r"\s+-\s*\d+\s+-\s*", " ", text)
        text = re.sub(r"\s+\d+\s+(?=\n|$)", " ", text)

        text = re.sub(r"Cá»˜NG HÃ’A XÃƒ Há»˜I CHá»¦ NGHÄ¨A VIá»†T NAM", "", text)
        text = re.sub(r"Äá»™c láº­p - Tá»± do - Háº¡nh phÃºc", "", text)

        # 4. Ná»‘i dÃ²ng (Text Reconstruction)
        # Thay tháº¿ xuá»‘ng dÃ²ng Ä‘Æ¡n láº» báº±ng khoáº£ng tráº¯ng (Ä‘á»ƒ ná»‘i cÃ¢u bá»‹ ngáº¯t)
        # Giá»¯ láº¡i xuá»‘ng dÃ²ng kÃ©p (Ä‘á»ƒ tÃ¡ch Ä‘oáº¡n)
        text = re.sub(r"(?<!\n)\n(?!\n)", " ", text)

        # 5. Chuáº©n hÃ³a khoáº£ng tráº¯ng (xÃ³a tab, space thá»«a)
        text = re.sub(r"\s+", " ", text)

        return text.strip()

    def get_pages_from_offset(self, start_idx, end_idx):
        """
        TÃ¬m xem Ä‘oáº¡n text tá»« start_idx Ä‘áº¿n end_idx náº±m trÃªn nhá»¯ng trang nÃ o
        dá»±a vÃ o self.page_offset_map.
        """
        pages = set()

        # Binary search hoáº·c duyá»‡t tuáº§n tá»± (duyá»‡t tuáº§n tá»± ok vÃ¬ sá»‘ trang Ã­t)
        # Tá»‘i Æ°u: Chá»‰ duyá»‡t cÃ¡c trang cÃ³ range giao vá»›i [start_idx, end_idx]
        for p_map in self.page_offset_map:
            # Kiá»ƒm tra giao nhau (Intersection)
            if not (end_idx <= p_map["start"] or start_idx >= p_map["end"]):
                pages.add(p_map["page"])

        return sorted(list(pages))

    def get_coordinates_by_offset(self, search_text, start_idx, end_idx):
        """
        Chá»‰ tÃ¬m kiáº¿m text trÃªn cÃ¡c trang Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi offset.
        """
        if not search_text:
            return [], []

        # 1. XÃ¡c Ä‘á»‹nh trang chá»©a Ä‘oáº¡n text nÃ y
        target_pages = self.get_pages_from_offset(start_idx, end_idx)

        if not target_pages:
            return [], []

        locations = []
        clean_search_key = re.sub(r"\s+", " ", search_text).strip()
        search_phrase = clean_search_key[:50]  # Láº¥y 50 chars Ä‘áº§u Ä‘á»ƒ search Rect

        # 2. Chá»‰ search trÃªn cÃ¡c trang Ä‘Ã­ch danh
        for page_num in target_pages:
            # Index máº£ng doc báº¯t Ä‘áº§u tá»« 0, page_num báº¯t Ä‘áº§u tá»« 1
            if page_num - 1 >= len(self.doc):
                continue

            page = self.doc[page_num - 1]
            quads = page.search_for(search_phrase)

            if quads:
                for q in quads:
                    locations.append(
                        {
                            "page": page_num,
                            "rect": [
                                round(q.x0, 2),
                                round(q.y0, 2),
                                round(q.x1, 2),
                                round(q.y1, 2),
                            ],
                        }
                    )

        return target_pages, locations

    # --- HÃ m helper Ä‘á»ƒ láº¥y footnote tá»« danh sÃ¡ch trang ---
    def _lookup_footnotes(self, page_numbers):
        """
        Input: List cÃ¡c sá»‘ trang [1, 2]
        Output: String gá»™p footnote (VD: "[Trang 1]: Note...\n[Trang 2]: Note...")
        """
        collected_notes = []
        for p in page_numbers:
            if p in self.page_footnotes_map:
                note_content = self.page_footnotes_map[p]
                if note_content:
                    collected_notes.append(f"[Trang {p}]: {note_content}")

        return "\n".join(collected_notes) if collected_notes else ""

    def recursive_split(self, article_dict, base_offset):
        """
        Chiáº¿n lÆ°á»£c: Structure-Based Chunking (Æ¯u tiÃªn cáº¥u trÃºc)
        - Äiá»u khÃ´ng cÃ³ khoáº£n (hoáº·c <= 5 khoáº£n) -> Giá»¯ nguyÃªn 1 chunk.
        - Khoáº£n khÃ´ng cÃ³ Ä‘iá»ƒm (hoáº·c <= 5 Ä‘iá»ƒm) -> Giá»¯ nguyÃªn chunk cáº¥p Khoáº£n.
        - Chá»‰ chia nhá» khi sá»‘ lÆ°á»£ng sub-items > 5.
        """
        full_text = article_dict["content"]
        article_title = article_dict["title"]  # VD: "Äiá»u 79. Thu há»“i Ä‘áº¥t..."
        article_id = article_dict["id"]

        # Regex tÃ¬m khoáº£n: "1. ", "2. " á»Ÿ Ä‘áº§u dÃ²ng hoáº·c sau dáº¥u xuá»‘ng dÃ²ng
        clause_pattern = r"(?m)(^|\n)(\d+)\.\s"
        matches = list(re.finditer(clause_pattern, full_text))

        # --- LOGIC 1: ÄIá»€U KIá»†N Cáº®T (ADAPTIVE) ---
        # Cáº¯t náº¿u: DÃ i > 1500 kÃ½ tá»± HOáº¶C cÃ³ > 5 khoáº£n (giáº£m ngÆ°á»¡ng xuá»‘ng 5 Ä‘á»ƒ an toÃ n hÆ¡n)
        should_split = len(matches) > 5

        if not should_split:
            # Case A: Giá»¯ nguyÃªn (Full Article Chunk)

            # 1. Táº¡o text sáº¡ch Ä‘á»ƒ lÆ°u DB
            final_db_text = self.clean_text_for_embedding(
                f"{article_title} | {full_text}"
            )

            # TÃ­nh offset tuyá»‡t Ä‘á»‘i cá»§a Ä‘oáº¡n text nÃ y
            # LÆ°u Ã½: full_text á»Ÿ Ä‘Ã¢y lÃ  article body, nÃªn tá»a Ä‘á»™ thá»±c táº¿ = base_offset
            # Tuy nhiÃªn Ä‘á»ƒ search chÃ­nh xÃ¡c title + body thÃ¬ hÆ¡i khÃ³ vÃ¬ title Ä‘Ã£ bá»‹ tÃ¡ch.
            # DÃ¹ng 100 kÃ½ tá»± Ä‘áº§u cá»§a body Ä‘á»ƒ tÃ¬m trang.

            abs_start = base_offset
            abs_end = base_offset + len(full_text)

            pgs, coords = self.get_coordinates_by_offset(
                full_text[:100], abs_start, abs_end
            )

            footnotes_str = self._lookup_footnotes(pgs)  # Tra cá»©u footnote

            chunk_id = f"law_{self.law_id}_art_{article_id}"
            return [
                {
                    "page_content": final_db_text,
                    "metadata": {
                        **article_dict["metadata"],
                        "chunk_id": chunk_id,
                        "chunk_type": "full_article",
                        "clause_id": None,
                        "point_id": None,
                        "page_number": pgs,
                        "coordinates": coords,
                        "chunk_footnotes": footnotes_str,
                    },
                }
            ]

        # Case B: Cáº¯t nhá» (Recursive Logic)
        results = []

        # 1. TÃ¡ch Preamble (Lá»i dáº«n) cáº¥p Äiá»u
        if matches:
            first_match_start = matches[0].start()
            article_preamble = full_text[:first_match_start].strip()
        else:
            article_preamble = ""

        # Duyá»‡t qua tá»«ng khoáº£n
        for i, match in enumerate(matches):
            clause_id = match.group(2)
            start = match.end()
            # Äiá»ƒm cuá»‘i lÃ  Ä‘iá»ƒm Ä‘áº§u cá»§a khoáº£n tiáº¿p theo, hoáº·c háº¿t vÄƒn báº£n
            end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)
            clause_content = full_text[start:end].strip()

            # TÃ­nh offset tuyá»‡t Ä‘á»‘i trong file gá»‘c
            # match.end() lÃ  vá»‹ trÃ­ sau "1. ", cáº§n cá»™ng vá»›i base_offset cá»§a Article
            abs_start = base_offset + start
            abs_end = base_offset + end

            # --- LOGIC 2: Xá»¬ LÃ ÄIá»‚M (ADAPTIVE SUB-SPLITTING) ---
            # ThÃªm 'Ä‘' vÃ o regex cho tiáº¿ng Viá»‡t
            point_pattern = r"(?m)(^|\n|\s)([a-zÄ‘])\)\s"
            point_matches = list(re.finditer(point_pattern, clause_content))

            # Äiá»u kiá»‡n tÃ¡ch Ä‘iá»ƒm: CÃ³ Ä‘iá»ƒm VÃ€ (Nhiá»u Ä‘iá»ƒm > 5 HOáº¶C Ná»™i dung quÃ¡ dÃ i > 800)
            has_points = len(point_matches) > 0
            should_split_points = has_points and (len(point_matches) > 5)

            if not should_split_points:
                # TRÆ¯á»œNG Há»¢P Gá»˜P (MERGE): Táº¡o Chunk cáº¥p Khoáº£n

                # Prepend context: TiÃªu Ä‘á» + Lá»i dáº«n Äiá»u
                # LÆ°u Ã½: article_preamble cáº§n Ä‘Æ°á»£c clean nháº¹ Ä‘á»ƒ ná»‘i chuá»—i Ä‘áº¹p hÆ¡n
                clean_art_preamble = self.clean_text_for_embedding(article_preamble)

                full_chunk_text = f"{article_title} | {clean_art_preamble} | Khoáº£n {clause_id}: {clause_content}"
                final_db_text = self.clean_text_for_embedding(full_chunk_text)

                # TÃ¬m tá»a Ä‘á»™ dÃ¹ng offset tuyá»‡t Ä‘á»‘i
                pgs, coords = self.get_coordinates_by_offset(
                    clause_content[:100], abs_start, abs_end
                )
                footnotes_str = self._lookup_footnotes(pgs)

                chunk_id = f"law_{self.law_id}_art_{article_id}_clause_{clause_id}"
                results.append(
                    {
                        "page_content": final_db_text,
                        "metadata": {
                            **article_dict["metadata"],
                            "chunk_id": chunk_id,
                            "chunk_type": "clause",
                            "clause_id": clause_id,
                            "point_id": None,
                            "page_number": pgs,
                            "coordinates": coords,
                            "has_points": has_points,  # Flag Ä‘Ã¡nh dáº¥u
                            "chunk_footnotes": footnotes_str,
                        },
                    }
                )
            else:
                # TRÆ¯á»œNG Há»¢P TÃCH (SPLIT): Cáº¯t sÃ¢u xuá»‘ng cáº¥p Äiá»ƒm

                # TÃ¡ch Preamble cáº¥p Khoáº£n
                clause_preamble = clause_content[: point_matches[0].start()].strip()
                clean_clause_preamble = self.clean_text_for_embedding(clause_preamble)
                clean_art_preamble = self.clean_text_for_embedding(article_preamble)

                for j, p_match in enumerate(point_matches):
                    point_id = p_match.group(2)
                    p_start = p_match.end()
                    p_end = (
                        point_matches[j + 1].start()
                        if j + 1 < len(point_matches)
                        else len(clause_content)
                    )
                    point_content = clause_content[p_start:p_end].strip()

                    # Offset tuyá»‡t Ä‘á»‘i cá»§a Point
                    # = Base Article + Rel Start Clause + Rel Start Point
                    p_abs_start = abs_start + p_start
                    p_abs_end = abs_start + p_end

                    # Táº¡o Chunk Äiá»ƒm (Full Context)
                    # Prepend context: TiÃªu Ä‘á» + Lá»i dáº«n Äiá»u + Khoáº£n sá»‘ + Lá»i dáº«n Khoáº£n
                    full_chunk_text = f"{article_title} | {clean_art_preamble} | Khoáº£n {clause_id}: {clean_clause_preamble} | Äiá»ƒm {point_id}) {point_content}"
                    final_db_text = self.clean_text_for_embedding(full_chunk_text)

                    # TÃ¬m tá»a Ä‘á»™ Ä‘iá»ƒm
                    pgs, coords = self.get_coordinates_by_offset(
                        point_content[:100], p_abs_start, p_abs_end
                    )
                    footnotes_str = self._lookup_footnotes(pgs)

                    chunk_id = f"law_{self.law_id}_art_{article_id}_clause_{clause_id}_point_{point_id}"
                    results.append(
                        {
                            "page_content": final_db_text,
                            "metadata": {
                                **article_dict["metadata"],
                                "chunk_id": chunk_id,
                                "chunk_type": "point",
                                "clause_id": clause_id,
                                "point_id": point_id,
                                "page_number": pgs,
                                "coordinates": coords,
                                "chunk_footnotes": footnotes_str,
                            },
                        }
                    )

        return results

    def _extract_article_info(self, raw_article_text):
        """
        HÃ m helper: TÃ¡ch text thÃ´ cá»§a má»™t Äiá»u luáº­t thÃ nh 3 pháº§n:
        1. art_id: Sá»‘ hiá»‡u Ä‘iá»u (VD: "7")
        2. title: TÃªn Ä‘iá»u Ä‘Ã£ Ä‘Æ°á»£c ná»‘i dÃ²ng hoÃ n chá»‰nh.
        3. body: Ná»™i dung chi tiáº¿t (báº¯t Ä‘áº§u tá»« Khoáº£n 1 hoáº·c ná»™i dung Ä‘iá»u Ä‘Æ¡n).
        """
        # 1. TÃ¡ch sá»‘ hiá»‡u Ä‘iá»u
        first_line_match = re.search(r"Äiá»u\s+(\d+)", raw_article_text)
        if not first_line_match:
            return None, None, None

        art_id = first_line_match.group(1)

        # 2. Chiáº¿n thuáº­t tÃ¡ch Title thÃ´ng minh
        # Æ¯u tiÃªn: TÃ¬m "Khoáº£n 1." hoáº·c "1." lÃ m má»‘c phÃ¢n chia Title vÃ  Body
        clause_1_match = re.search(r"(\n|^)1\.\s", raw_article_text)

        full_art_title = ""
        content_body = ""
        # Offset tÆ°Æ¡ng Ä‘á»‘i nÆ¡i body báº¯t Ä‘áº§u (Ä‘á»ƒ cá»™ng bÃ¹ trá»« náº¿u cáº§n, á»Ÿ Ä‘Ã¢y chÆ°a cáº§n thiáº¿t láº¯m)
        body_start_rel_offset = 0

        if clause_1_match:
            # --- TRÆ¯á»œNG Há»¢P A: Äiá»u luáº­t CÃ“ chia khoáº£n (Äiá»u 7, Äiá»u 3...) ---
            split_idx = clause_1_match.start()
            if clause_1_match.group(1) == "\n":
                split_idx += 1

            title_segment = raw_article_text[:split_idx].strip()
            # Ná»‘i cÃ¡c dÃ²ng title bá»‹ ngáº¯t (word wrap) thÃ nh 1 dÃ²ng
            full_art_title = title_segment.replace("\n", " ")

            # Ná»™i dung body giá»¯ nguyÃªn tá»« "1. ..."
            content_body = raw_article_text[split_idx:].strip()
            body_start_rel_offset = split_idx

        else:
            # --- TRÆ¯á»œNG Há»¢P B: Äiá»u luáº­t KHÃ”NG chia khoáº£n (Äiá»u 1, Äiá»u 2...) ---
            lines = raw_article_text.split("\n")
            if not lines:
                return art_id, "", ""

            # DÃ²ng Ä‘áº§u tiÃªn cháº¯c cháº¯n lÃ  má»™t pháº§n cá»§a title
            title_parts = [lines[0].strip()]
            body_start_idx = 1

            # Check cÃ¡c dÃ²ng tiáº¿p theo
            for k in range(1, len(lines)):
                line = lines[k].strip()
                if not line:
                    continue

                # Náº¿u dÃ²ng báº¯t Ä‘áº§u báº±ng chá»¯ thÆ°á»ng -> LÃ  pháº§n dÆ° cá»§a title dÃ²ng trÃªn
                if line[0].islower():
                    title_parts.append(line)
                    body_start_idx = k + 1
                else:
                    # Gáº·p chá»¯ Hoa hoáº·c Sá»‘ -> Báº¯t Ä‘áº§u ná»™i dung Body -> Dá»«ng
                    break

            full_art_title = " ".join(title_parts)
            content_body = "\n".join(lines[body_start_idx:]).strip()

            # TÃ­nh offset tÆ°Æ¡ng Ä‘á»‘i (tÆ°Æ¡ng Ä‘á»‘i)
            # Find location of content body in raw text to be precise
            idx = raw_article_text.find(content_body)
            if idx != -1:
                body_start_rel_offset = idx

        # Clean up láº§n cuá»‘i
        full_art_title = re.sub(r"\s+", " ", full_art_title).strip()

        # Fallback: Náº¿u body rá»—ng (lá»—i format), láº¥y title lÃ m body Ä‘á»ƒ khÃ´ng máº¥t dá»¯ liá»‡u
        if not content_body:
            content_body = full_art_title

        return art_id, full_art_title, content_body, body_start_rel_offset

    def process(self):
        print(f"ğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½ file: {self.pdf_path}")

        # Determine actual pages to process
        total_pages = len(self.doc)
        pages_to_process = (
            min(self.max_pages, total_pages) if self.max_pages else total_pages
        )

        if self.max_pages:
            print(f"ğŸ“‹ Giá»›i háº¡n xá»­ lÃ½: {pages_to_process}/{total_pages} trang")

        full_text = ""
        current_offset = 0
        self.page_offset_map = []  # Reset map
        print(f"ğŸ“„ Äang Ä‘á»c PDF: TÃ¡ch ná»™i dung chÃ­nh vÃ  Footnote...")

        # 1. Duyá»‡t qua tá»«ng trang Ä‘á»ƒ tÃ¡ch Content vÃ  Footnote ngay tá»« Ä‘áº§u
        for i, page in enumerate(self.doc):
            if i >= pages_to_process:
                break

            page_num = i + 1
            clean, note = self.get_page_content_and_footnotes(page)

            start_pos = current_offset
            # LÆ°u Ã½: clean string + "\n"
            text_len = len(clean) + 1
            end_pos = start_pos + text_len

            # LÆ°u map: Trang i+1 chá»©a text tá»« start_pos Ä‘áº¿n end_pos
            self.page_offset_map.append(
                {"page": i + 1, "start": start_pos, "end": end_pos}
            )

            full_text += clean + "\n"
            current_offset = end_pos

            # LÆ°u footnote vÃ o map náº¿u cÃ³
            if note:
                self.page_footnotes_map[page_num] = note

        print(f"âœ“ ÄÃ£ Ä‘á»c xong {pages_to_process} trang. ÄÃ£ lÆ°u index Footnote.")

        # 2. Pattern báº¯t cÃ¡c tiÃªu Ä‘á» cáº¥u trÃºc (Hierarchy)
        # Regex nÃ y tÃ¬m dÃ²ng báº¯t Ä‘áº§u báº±ng ChÆ°Æ¡ng, Má»¥c hoáº·c Äiá»u
        hierarchy_pattern = (
            r"(?m)^(ChÆ°Æ¡ng\s+[IVXLCDM]+|Má»¥c\s+\d+|Äiá»u\s+(\d+)\.)\s+(.*)"
        )

        matches = list(re.finditer(hierarchy_pattern, full_text))
        self.log_structure_hierarchy(matches)
        print(f"â³ Báº¯t Ä‘áº§u xá»­ lÃ½ chi tiáº¿t...\n")

        for i, match in enumerate(matches):
            marker_type = match.group(1)  # ChÆ°Æ¡ng I, Má»¥c 1, Äiá»u 1.
            content_title = match.group(3).strip()

            # Progress indicator every 10 items or for articles
            if i % 10 == 0 or marker_type.startswith("Äiá»u"):
                progress_pct = (i / len(matches)) * 100
                print(
                    f"ğŸ“ [{i+1}/{len(matches)} - {progress_pct:.1f}%] {marker_type} - {content_title[:60]}..."
                )

            # --- Cáº¬P NHáº¬T TRáº NG THÃI (State Machine) ---
            if marker_type.startswith("ChÆ°Æ¡ng"):
                parts = marker_type.split()
                c_id = parts[1] if len(parts) > 1 else "Unknown"
                self.current_chapter = {"id": c_id, "title": content_title}
                self.current_section = {
                    "id": None,
                    "title": None,
                }  # RESET SECTION QUAN TRá»ŒNG

            elif marker_type.startswith("Má»¥c"):
                parts = marker_type.split()
                s_id = parts[1] if len(parts) > 1 else "Unknown"
                self.current_section = {"id": s_id, "title": content_title}

            elif marker_type.startswith("Äiá»u"):
                end_idx = (
                    matches[i + 1].start() if i + 1 < len(matches) else len(full_text)
                )

                # Offset báº¯t Ä‘áº§u cá»§a Äiá»u trong toÃ n bá»™ vÄƒn báº£n
                article_global_start = match.start()

                raw_article_text = full_text[article_global_start:end_idx].strip()
                art_id, full_art_title, content_body, body_rel_offset = (
                    self._extract_article_info(raw_article_text)
                )

                if not art_id:
                    continue  # Bá» qua náº¿u khÃ´ng tÃ¬m tháº¥y ID

                # Metadata cho Äiá»u
                meta = {
                    "law_id": self.law_id,
                    "chapter_id": self.current_chapter["id"],
                    "chapter_title": self.current_chapter["title"],
                    "section_id": self.current_section["id"],
                    "section_title": self.current_section["title"],
                    "article_id": art_id,
                    "article_title": full_art_title,
                    "topic": "legal_document",  # Placeholder
                    "source_file": self.pdf_path.split("/")[-1],
                    "footnotes": "",  # Placeholder cho tÆ°Æ¡ng lai
                }

                # Tinh chá»‰nh offset truyá»n vÃ o recursive_split
                # recursive_split xá»­ lÃ½ trÃªn content_body, nÃªn base_offset pháº£i cá»™ng thÃªm pháº§n tiÃªu Ä‘á» Ä‘Ã£ cáº¯t
                final_body_offset = article_global_start + body_rel_offset

                # Gá»i hÃ m cáº¯t
                chunks = self.recursive_split(
                    {
                        "id": art_id,
                        "title": full_art_title,
                        "content": content_body,
                        "metadata": meta,
                    },
                    base_offset=final_body_offset,  # [FIX] Truyá»n offset chÃ­nh xÃ¡c
                )
                self.chunks.extend(chunks)
                print(
                    f"   âœ“ Äiá»u {art_id}: {len(chunks)} chunks | Tá»•ng: {len(self.chunks)}"
                )

            # Checkpoint every 50 items
            if i > 0 and (i + 1) % 50 == 0:
                print(
                    f"\nğŸ¯ Checkpoint: {i+1}/{len(matches)} ({(i+1)/len(matches)*100:.1f}%) - {len(self.chunks)} chunks tá»•ng\n"
                )

        print(f"\nâœ… HoÃ n thÃ nh! Tá»•ng cá»™ng {len(self.chunks)} chunks Ä‘Æ°á»£c táº¡o ra.")
        return self.chunks


# ==========================================
# TEST RUNNER (Äá»ƒ báº¡n cháº¡y thá»­)
# ==========================================
if __name__ == "__main__":
    # Thay tÃªn file PDF cá»§a báº¡n vÃ o Ä‘Ã¢y
    PDF_FILE = "./data/133-vbhn-vpqh.pdf"

    try:
        parser = LandLawChunkerFinal(PDF_FILE)
        final_data = parser.process()

        # Page 218 is image, so fitz can not load content, temp handle this way
        # Remove the last item from final_data
        if final_data:
            final_data.pop()
            print(f"ğŸ—‘ï¸ ÄÃ£ xÃ³a item cuá»‘i cÃ¹ng. CÃ²n láº¡i: {len(final_data)} chunks")

        # Load and concatenate content from law_content_page_128.json
        EXTERNAL_JSON_FILE = "./data/law_content_page_128.json"
        final_data = load_and_concatenate_external_json(EXTERNAL_JSON_FILE, final_data)

        # Xuáº¥t káº¿t quáº£
        OUTPUT_FILE = "./data/land_law_chunks_final.json"
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {OUTPUT_FILE}")

    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
