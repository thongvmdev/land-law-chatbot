import fitz  # PyMuPDF
import re
import json


class LandLawChunkerFinal:
    def __init__(self, pdf_path, max_pages=None):
        self.pdf_path = pdf_path
        self.max_pages = max_pages
        try:
            self.doc = fitz.open(pdf_path)
        except Exception as e:
            raise ValueError(f"Kh√¥ng th·ªÉ m·ªü file PDF: {e}")

        self.law_id = "133/VBHN-VPQH"
        self.chunks = []

        # State variables (Hierarchy)
        self.current_chapter = {"id": None, "title": None}
        self.current_section = {"id": None, "title": None}

        # [M·ªöI] Kho l∆∞u tr·ªØ Footnote theo trang: { page_num: "n·ªôi dung footnote" }
        self.page_footnotes_map = {}

        # [M·ªöI] B·∫£n ƒë·ªì √°nh x·∫° t·ª´ Index trong full_text sang S·ªë trang
        # Format: [{"page": 1, "start": 0, "end": 1000}, ...]
        self.page_offset_map = []

    def get_page_content_and_footnotes(self, page):
        """
        Tr·∫£ v·ªÅ 2 gi√° tr·ªã:
        1. clean_text: N·ªôi dung ch√≠nh (c·ª° ch·ªØ to)
        2. footnote_text: N·ªôi dung ch√∫ th√≠ch (c·ª° ch·ªØ nh·ªè)
        """

        blocks = page.get_text(
            "dict",
            flags=fitz.TEXT_PRESERVE_LIGATURES | fitz.TEXT_PRESERVE_WHITESPACE,
        )["blocks"]
        clean_text = ""
        footnote_text = ""

        # Ng∆∞·ª°ng c·ª° ch·ªØ ph√¢n lo·∫°i
        FONT_SIZE_THRESHOLD = 12

        for b in blocks:
            if "lines" in b:
                for l in b["lines"]:
                    line_clean = ""
                    line_note = ""
                    for s in l["spans"]:
                        text_segment = s["text"]
                        if s["size"] > FONT_SIZE_THRESHOLD:
                            line_clean += text_segment
                        else:
                            # L·ªçc r√°c: B·ªè qua s·ªë trang ƒë∆°n l·∫ª n·∫øu n√≥ l·∫´n v√†o footnote
                            if not re.match(r"^\s*\d+\s*$", text_segment):
                                line_note += text_segment

                    if line_clean.strip():
                        clean_text += line_clean + "\n"
                    if line_note.strip():
                        footnote_text += line_note + " "  # N·ªëi footnote th√†nh d√≤ng d√†i

        return clean_text, footnote_text.strip()

    def log_structure_hierarchy(self, matches):
        """
        In ra c·∫•u tr√∫c c√¢y c·ªßa vƒÉn b·∫£n lu·∫≠t d·ª±a tr√™n k·∫øt qu·∫£ Regex.
        """
        print(f"\nüîç T√¨m th·∫•y {len(matches)} ƒëi·ªÉm ƒë√°nh d·∫•u c·∫•u tr√∫c.")
        print("=" * 60)
        print(f"{'LO·∫†I':<10} | {'CHI TI·∫æT':<50}")
        print("=" * 60)

        count_chuong = 0
        count_muc = 0
        count_dieu = 0

        for m in matches:
            marker = m.group(1).strip()  # VD: Ch∆∞∆°ng I, M·ª•c 1, ƒêi·ªÅu 1.
            title = m.group(3).strip()  # VD: Ph·∫°m vi ƒëi·ªÅu ch·ªânh

            if marker.startswith("Ch∆∞∆°ng"):
                count_chuong += 1
                print(f"üìò {marker}: {title.upper()}")
            elif marker.startswith("M·ª•c"):
                count_muc += 1
                print(f"  üìÇ {marker}: {title}")
            elif marker.startswith("ƒêi·ªÅu"):
                count_dieu += 1
                display_title = (title[:50] + "...") if len(title) > 50 else title
                print(f"    üìÑ {marker} {display_title}")

        print("=" * 60)
        print(
            f"üìä TH·ªêNG K√ä: {count_chuong} Ch∆∞∆°ng | {count_muc} M·ª•c | {count_dieu} ƒêi·ªÅu"
        )
        print("=" * 60 + "\n")

    def clean_text_for_embedding(self, text):
        """
        L√†m s·∫°ch text tri·ªát ƒë·ªÉ ƒë·ªÉ l∆∞u v√†o DB (d√πng cho semantic search).
        """
        # 1. X√≥a ƒë√°nh d·∫•u trang v√† header/footer c·ªë ƒë·ªãnh
        # X√≥a d√≤ng "--- PAGE 123 ---"
        text = re.sub(r"--- PAGE \d+ ---", "", text)
        # X√≥a c√°c s·ªë trang ƒë∆°n l·∫ª ·ªü ƒë·∫ßu/cu·ªëi d√≤ng (th∆∞·ªùng l√† s·ªë trang)
        text = re.sub(r"^\s*\d+\s*$", "", text, flags=re.MULTILINE)
        # X√≥a s·ªë trang ·ªü gi·ªØa d√≤ng v·ªõi format " - 123 - " ho·∫∑c " 123 "
        text = re.sub(r"\s+-\s*\d+\s+-\s*", " ", text)
        text = re.sub(r"\s+\d+\s+(?=\n|$)", " ", text)

        text = re.sub(r"C·ªòNG H√íA X√É H·ªòI CH·ª¶ NGHƒ®A VI·ªÜT NAM", "", text)
        text = re.sub(r"ƒê·ªôc l·∫≠p - T·ª± do - H·∫°nh ph√∫c", "", text)

        # 4. N·ªëi d√≤ng (Text Reconstruction)
        # Thay th·∫ø xu·ªëng d√≤ng ƒë∆°n l·∫ª b·∫±ng kho·∫£ng tr·∫Øng (ƒë·ªÉ n·ªëi c√¢u b·ªã ng·∫Øt)
        # Gi·ªØ l·∫°i xu·ªëng d√≤ng k√©p (ƒë·ªÉ t√°ch ƒëo·∫°n)
        text = re.sub(r"(?<!\n)\n(?!\n)", " ", text)

        # 5. Chu·∫©n h√≥a kho·∫£ng tr·∫Øng (x√≥a tab, space th·ª´a)
        text = re.sub(r"\s+", " ", text)

        return text.strip()

    def get_pages_from_offset(self, start_idx, end_idx):
        """
        T√¨m xem ƒëo·∫°n text t·ª´ start_idx ƒë·∫øn end_idx n·∫±m tr√™n nh·ªØng trang n√†o
        d·ª±a v√†o self.page_offset_map.
        """
        pages = set()

        # Binary search ho·∫∑c duy·ªát tu·∫ßn t·ª± (duy·ªát tu·∫ßn t·ª± ok v√¨ s·ªë trang √≠t)
        # T·ªëi ∆∞u: Ch·ªâ duy·ªát c√°c trang c√≥ range giao v·ªõi [start_idx, end_idx]
        for p_map in self.page_offset_map:
            # Ki·ªÉm tra giao nhau (Intersection)
            if not (end_idx <= p_map["start"] or start_idx >= p_map["end"]):
                pages.add(p_map["page"])

        return sorted(list(pages))

    def get_coordinates_by_offset(self, search_text, start_idx, end_idx):
        """
        Ch·ªâ t√¨m ki·∫øm text tr√™n c√°c trang ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi offset.
        """
        if not search_text:
            return [], []

        # 1. X√°c ƒë·ªãnh trang ch·ª©a ƒëo·∫°n text n√†y
        target_pages = self.get_pages_from_offset(start_idx, end_idx)

        if not target_pages:
            return [], []

        locations = []
        clean_search_key = re.sub(r"\s+", " ", search_text).strip()
        search_phrase = clean_search_key[:50]  # L·∫•y 50 chars ƒë·∫ßu ƒë·ªÉ search Rect

        # 2. Ch·ªâ search tr√™n c√°c trang ƒë√≠ch danh
        for page_num in target_pages:
            # Index m·∫£ng doc b·∫Øt ƒë·∫ßu t·ª´ 0, page_num b·∫Øt ƒë·∫ßu t·ª´ 1
            if page_num - 1 >= len(self.doc):
                continue

            page = self.doc[page_num - 1]
            quads = page.search_for(search_phrase)

            if quads:
                for q in quads:
                    locations.append(
                        {
                            "page": page_num,
                            "rect": [
                                round(q.x0, 2),
                                round(q.y0, 2),
                                round(q.x1, 2),
                                round(q.y1, 2),
                            ],
                        }
                    )

        return target_pages, locations

    # --- H√†m helper ƒë·ªÉ l·∫•y footnote t·ª´ danh s√°ch trang ---
    def _lookup_footnotes(self, page_numbers):
        """
        Input: List c√°c s·ªë trang [1, 2]
        Output: String g·ªôp footnote (VD: "[Trang 1]: Note...\n[Trang 2]: Note...")
        """
        collected_notes = []
        for p in page_numbers:
            if p in self.page_footnotes_map:
                note_content = self.page_footnotes_map[p]
                if note_content:
                    collected_notes.append(f"[Trang {p}]: {note_content}")

        return "\n".join(collected_notes) if collected_notes else ""

    def recursive_split(self, article_dict, base_offset):
        """
        Chi·∫øn l∆∞·ª£c: Structure-Based Chunking (∆Øu ti√™n c·∫•u tr√∫c)
        - ƒêi·ªÅu kh√¥ng c√≥ kho·∫£n (ho·∫∑c <= 5 kho·∫£n) -> Gi·ªØ nguy√™n 1 chunk.
        - Kho·∫£n kh√¥ng c√≥ ƒëi·ªÉm (ho·∫∑c <= 5 ƒëi·ªÉm) -> Gi·ªØ nguy√™n chunk c·∫•p Kho·∫£n.
        - Ch·ªâ chia nh·ªè khi s·ªë l∆∞·ª£ng sub-items > 5.
        """
        full_text = article_dict["content"]
        article_title = article_dict["title"]  # VD: "ƒêi·ªÅu 79. Thu h·ªìi ƒë·∫•t..."
        article_id = article_dict["id"]

        # Regex t√¨m kho·∫£n: "1. ", "2. " ·ªü ƒë·∫ßu d√≤ng ho·∫∑c sau d·∫•u xu·ªëng d√≤ng
        clause_pattern = r"(?m)(^|\n)(\d+)\.\s"
        matches = list(re.finditer(clause_pattern, full_text))

        # --- LOGIC 1: ƒêI·ªÄU KI·ªÜN C·∫ÆT (ADAPTIVE) ---
        # C·∫Øt n·∫øu: D√†i > 1500 k√Ω t·ª± HO·∫∂C c√≥ > 5 kho·∫£n (gi·∫£m ng∆∞·ª°ng xu·ªëng 5 ƒë·ªÉ an to√†n h∆°n)
        should_split = len(matches) > 5

        if not should_split:
            # Case A: Gi·ªØ nguy√™n (Full Article Chunk)

            # 1. T·∫°o text s·∫°ch ƒë·ªÉ l∆∞u DB
            final_db_text = self.clean_text_for_embedding(
                f"{article_title} | {full_text}"
            )

            # T√≠nh offset tuy·ªát ƒë·ªëi c·ªßa ƒëo·∫°n text n√†y
            # L∆∞u √Ω: full_text ·ªü ƒë√¢y l√† article body, n√™n t·ªça ƒë·ªô th·ª±c t·∫ø = base_offset
            # Tuy nhi√™n ƒë·ªÉ search ch√≠nh x√°c title + body th√¨ h∆°i kh√≥ v√¨ title ƒë√£ b·ªã t√°ch.
            # D√πng 100 k√Ω t·ª± ƒë·∫ßu c·ªßa body ƒë·ªÉ t√¨m trang.

            abs_start = base_offset
            abs_end = base_offset + len(full_text)

            pgs, coords = self.get_coordinates_by_offset(
                full_text[:100], abs_start, abs_end
            )

            footnotes_str = self._lookup_footnotes(pgs)  # Tra c·ª©u footnote

            chunk_id = f"law_{self.law_id}_art_{article_id}"
            return [
                {
                    "page_content": final_db_text,
                    "metadata": {
                        **article_dict["metadata"],
                        "chunk_id": chunk_id,
                        "chunk_type": "full_article",
                        "clause_id": None,
                        "point_id": None,
                        "page_number": pgs,
                        "coordinates": coords,
                        "chunk_footnotes": footnotes_str,
                    },
                }
            ]

        # Case B: C·∫Øt nh·ªè (Recursive Logic)
        results = []

        # 1. T√°ch Preamble (L·ªùi d·∫´n) c·∫•p ƒêi·ªÅu
        if matches:
            first_match_start = matches[0].start()
            article_preamble = full_text[:first_match_start].strip()
        else:
            article_preamble = ""

        # Duy·ªát qua t·ª´ng kho·∫£n
        for i, match in enumerate(matches):
            clause_id = match.group(2)
            start = match.end()
            # ƒêi·ªÉm cu·ªëi l√† ƒëi·ªÉm ƒë·∫ßu c·ªßa kho·∫£n ti·∫øp theo, ho·∫∑c h·∫øt vƒÉn b·∫£n
            end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)
            clause_content = full_text[start:end].strip()

            # T√≠nh offset tuy·ªát ƒë·ªëi trong file g·ªëc
            # match.end() l√† v·ªã tr√≠ sau "1. ", c·∫ßn c·ªông v·ªõi base_offset c·ªßa Article
            abs_start = base_offset + start
            abs_end = base_offset + end

            # --- LOGIC 2: X·ª¨ L√ù ƒêI·ªÇM (ADAPTIVE SUB-SPLITTING) ---
            # Th√™m 'ƒë' v√†o regex cho ti·∫øng Vi·ªát
            point_pattern = r"(?m)(^|\n|\s)([a-zƒë])\)\s"
            point_matches = list(re.finditer(point_pattern, clause_content))

            # ƒêi·ªÅu ki·ªán t√°ch ƒëi·ªÉm: C√≥ ƒëi·ªÉm V√Ä (Nhi·ªÅu ƒëi·ªÉm > 5 HO·∫∂C N·ªôi dung qu√° d√†i > 800)
            has_points = len(point_matches) > 0
            should_split_points = has_points and (len(point_matches) > 5)

            if not should_split_points:
                # TR∆Ø·ªúNG H·ª¢P G·ªòP (MERGE): T·∫°o Chunk c·∫•p Kho·∫£n

                # Prepend context: Ti√™u ƒë·ªÅ + L·ªùi d·∫´n ƒêi·ªÅu
                # L∆∞u √Ω: article_preamble c·∫ßn ƒë∆∞·ª£c clean nh·∫π ƒë·ªÉ n·ªëi chu·ªói ƒë·∫πp h∆°n
                clean_art_preamble = self.clean_text_for_embedding(article_preamble)

                full_chunk_text = f"{article_title} | {clean_art_preamble} | Kho·∫£n {clause_id}: {clause_content}"
                final_db_text = self.clean_text_for_embedding(full_chunk_text)

                # T√¨m t·ªça ƒë·ªô d√πng offset tuy·ªát ƒë·ªëi
                pgs, coords = self.get_coordinates_by_offset(
                    clause_content[:100], abs_start, abs_end
                )
                footnotes_str = self._lookup_footnotes(pgs)

                chunk_id = f"law_{self.law_id}_art_{article_id}_clause_{clause_id}"
                results.append(
                    {
                        "page_content": final_db_text,
                        "metadata": {
                            **article_dict["metadata"],
                            "chunk_id": chunk_id,
                            "chunk_type": "clause",
                            "clause_id": clause_id,
                            "point_id": None,
                            "page_number": pgs,
                            "coordinates": coords,
                            "has_points": has_points,  # Flag ƒë√°nh d·∫•u
                            "chunk_footnotes": footnotes_str,
                        },
                    }
                )
            else:
                # TR∆Ø·ªúNG H·ª¢P T√ÅCH (SPLIT): C·∫Øt s√¢u xu·ªëng c·∫•p ƒêi·ªÉm

                # T√°ch Preamble c·∫•p Kho·∫£n
                clause_preamble = clause_content[: point_matches[0].start()].strip()
                clean_clause_preamble = self.clean_text_for_embedding(clause_preamble)
                clean_art_preamble = self.clean_text_for_embedding(article_preamble)

                for j, p_match in enumerate(point_matches):
                    point_id = p_match.group(2)
                    p_start = p_match.end()
                    p_end = (
                        point_matches[j + 1].start()
                        if j + 1 < len(point_matches)
                        else len(clause_content)
                    )
                    point_content = clause_content[p_start:p_end].strip()

                    # Offset tuy·ªát ƒë·ªëi c·ªßa Point
                    # = Base Article + Rel Start Clause + Rel Start Point
                    p_abs_start = abs_start + p_start
                    p_abs_end = abs_start + p_end

                    # T·∫°o Chunk ƒêi·ªÉm (Full Context)
                    # Prepend context: Ti√™u ƒë·ªÅ + L·ªùi d·∫´n ƒêi·ªÅu + Kho·∫£n s·ªë + L·ªùi d·∫´n Kho·∫£n
                    full_chunk_text = f"{article_title} | {clean_art_preamble} | Kho·∫£n {clause_id}: {clean_clause_preamble} | ƒêi·ªÉm {point_id}) {point_content}"
                    final_db_text = self.clean_text_for_embedding(full_chunk_text)

                    # T√¨m t·ªça ƒë·ªô ƒëi·ªÉm
                    pgs, coords = self.get_coordinates_by_offset(
                        point_content[:100], p_abs_start, p_abs_end
                    )
                    footnotes_str = self._lookup_footnotes(pgs)

                    chunk_id = f"law_{self.law_id}_art_{article_id}_clause_{clause_id}_point_{point_id}"
                    results.append(
                        {
                            "page_content": final_db_text,
                            "metadata": {
                                **article_dict["metadata"],
                                "chunk_id": chunk_id,
                                "chunk_type": "point",
                                "clause_id": clause_id,
                                "point_id": point_id,
                                "page_number": pgs,
                                "coordinates": coords,
                                "chunk_footnotes": footnotes_str,
                            },
                        }
                    )

        return results

    def _extract_article_info(self, raw_article_text):
        """
        H√†m helper: T√°ch text th√¥ c·ªßa m·ªôt ƒêi·ªÅu lu·∫≠t th√†nh 3 ph·∫ßn:
        1. art_id: S·ªë hi·ªáu ƒëi·ªÅu (VD: "7")
        2. title: T√™n ƒëi·ªÅu ƒë√£ ƒë∆∞·ª£c n·ªëi d√≤ng ho√†n ch·ªânh.
        3. body: N·ªôi dung chi ti·∫øt (b·∫Øt ƒë·∫ßu t·ª´ Kho·∫£n 1 ho·∫∑c n·ªôi dung ƒëi·ªÅu ƒë∆°n).
        """
        # 1. T√°ch s·ªë hi·ªáu ƒëi·ªÅu
        first_line_match = re.search(r"ƒêi·ªÅu\s+(\d+)", raw_article_text)
        if not first_line_match:
            return None, None, None

        art_id = first_line_match.group(1)

        # 2. Chi·∫øn thu·∫≠t t√°ch Title th√¥ng minh
        # ∆Øu ti√™n: T√¨m "Kho·∫£n 1." ho·∫∑c "1." l√†m m·ªëc ph√¢n chia Title v√† Body
        clause_1_match = re.search(r"(\n|^)1\.\s", raw_article_text)

        full_art_title = ""
        content_body = ""
        # Offset t∆∞∆°ng ƒë·ªëi n∆°i body b·∫Øt ƒë·∫ßu (ƒë·ªÉ c·ªông b√π tr·ª´ n·∫øu c·∫ßn, ·ªü ƒë√¢y ch∆∞a c·∫ßn thi·∫øt l·∫Øm)
        body_start_rel_offset = 0

        if clause_1_match:
            # --- TR∆Ø·ªúNG H·ª¢P A: ƒêi·ªÅu lu·∫≠t C√ì chia kho·∫£n (ƒêi·ªÅu 7, ƒêi·ªÅu 3...) ---
            split_idx = clause_1_match.start()
            if clause_1_match.group(1) == "\n":
                split_idx += 1

            title_segment = raw_article_text[:split_idx].strip()
            # N·ªëi c√°c d√≤ng title b·ªã ng·∫Øt (word wrap) th√†nh 1 d√≤ng
            full_art_title = title_segment.replace("\n", " ")

            # N·ªôi dung body gi·ªØ nguy√™n t·ª´ "1. ..."
            content_body = raw_article_text[split_idx:].strip()
            body_start_rel_offset = split_idx

        else:
            # --- TR∆Ø·ªúNG H·ª¢P B: ƒêi·ªÅu lu·∫≠t KH√îNG chia kho·∫£n (ƒêi·ªÅu 1, ƒêi·ªÅu 2...) ---
            lines = raw_article_text.split("\n")
            if not lines:
                return art_id, "", ""

            # D√≤ng ƒë·∫ßu ti√™n ch·∫Øc ch·∫Øn l√† m·ªôt ph·∫ßn c·ªßa title
            title_parts = [lines[0].strip()]
            body_start_idx = 1

            # Check c√°c d√≤ng ti·∫øp theo
            for k in range(1, len(lines)):
                line = lines[k].strip()
                if not line:
                    continue

                # N·∫øu d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng ch·ªØ th∆∞·ªùng -> L√† ph·∫ßn d∆∞ c·ªßa title d√≤ng tr√™n
                if line[0].islower():
                    title_parts.append(line)
                    body_start_idx = k + 1
                else:
                    # G·∫∑p ch·ªØ Hoa ho·∫∑c S·ªë -> B·∫Øt ƒë·∫ßu n·ªôi dung Body -> D·ª´ng
                    break

            full_art_title = " ".join(title_parts)
            content_body = "\n".join(lines[body_start_idx:]).strip()

            # T√≠nh offset t∆∞∆°ng ƒë·ªëi (t∆∞∆°ng ƒë·ªëi)
            # Find location of content body in raw text to be precise
            idx = raw_article_text.find(content_body)
            if idx != -1:
                body_start_rel_offset = idx

        # Clean up l·∫ßn cu·ªëi
        full_art_title = re.sub(r"\s+", " ", full_art_title).strip()

        # Fallback: N·∫øu body r·ªóng (l·ªói format), l·∫•y title l√†m body ƒë·ªÉ kh√¥ng m·∫•t d·ªØ li·ªáu
        if not content_body:
            content_body = full_art_title

        return art_id, full_art_title, content_body, body_start_rel_offset

    def process(self):
        print(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω file: {self.pdf_path}")

        # Determine actual pages to process
        total_pages = len(self.doc)
        pages_to_process = (
            min(self.max_pages, total_pages) if self.max_pages else total_pages
        )

        if self.max_pages:
            print(f"üìã Gi·ªõi h·∫°n x·ª≠ l√Ω: {pages_to_process}/{total_pages} trang")

        full_text = ""
        current_offset = 0
        self.page_offset_map = []  # Reset map
        print(f"üìÑ ƒêang ƒë·ªçc PDF: T√°ch n·ªôi dung ch√≠nh v√† Footnote...")

        # 1. Duy·ªát qua t·ª´ng trang ƒë·ªÉ t√°ch Content v√† Footnote ngay t·ª´ ƒë·∫ßu
        for i, page in enumerate(self.doc):
            if i >= pages_to_process:
                break

            page_num = i + 1
            clean, note = self.get_page_content_and_footnotes(page)

            start_pos = current_offset
            # L∆∞u √Ω: clean string + "\n"
            text_len = len(clean) + 1
            end_pos = start_pos + text_len

            # L∆∞u map: Trang i+1 ch·ª©a text t·ª´ start_pos ƒë·∫øn end_pos
            self.page_offset_map.append(
                {"page": i + 1, "start": start_pos, "end": end_pos}
            )

            full_text += clean + "\n"
            current_offset = end_pos

            # L∆∞u footnote v√†o map n·∫øu c√≥
            if note:
                self.page_footnotes_map[page_num] = note

        print(f"‚úì ƒê√£ ƒë·ªçc xong {pages_to_process} trang. ƒê√£ l∆∞u index Footnote.")

        # 2. Pattern b·∫Øt c√°c ti√™u ƒë·ªÅ c·∫•u tr√∫c (Hierarchy)
        # Regex n√†y t√¨m d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng Ch∆∞∆°ng, M·ª•c ho·∫∑c ƒêi·ªÅu
        hierarchy_pattern = (
            r"(?m)^(Ch∆∞∆°ng\s+[IVXLCDM]+|M·ª•c\s+\d+|ƒêi·ªÅu\s+(\d+)\.)\s+(.*)"
        )

        matches = list(re.finditer(hierarchy_pattern, full_text))
        self.log_structure_hierarchy(matches)
        print(f"‚è≥ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω chi ti·∫øt...\n")

        for i, match in enumerate(matches):
            marker_type = match.group(1)  # Ch∆∞∆°ng I, M·ª•c 1, ƒêi·ªÅu 1.
            content_title = match.group(3).strip()

            # Progress indicator every 10 items or for articles
            if i % 10 == 0 or marker_type.startswith("ƒêi·ªÅu"):
                progress_pct = (i / len(matches)) * 100
                print(
                    f"üìç [{i+1}/{len(matches)} - {progress_pct:.1f}%] {marker_type} - {content_title[:60]}..."
                )

            # --- C·∫¨P NH·∫¨T TR·∫†NG TH√ÅI (State Machine) ---
            if marker_type.startswith("Ch∆∞∆°ng"):
                parts = marker_type.split()
                c_id = parts[1] if len(parts) > 1 else "Unknown"
                self.current_chapter = {"id": c_id, "title": content_title}
                self.current_section = {
                    "id": None,
                    "title": None,
                }  # RESET SECTION QUAN TR·ªåNG

            elif marker_type.startswith("M·ª•c"):
                parts = marker_type.split()
                s_id = parts[1] if len(parts) > 1 else "Unknown"
                self.current_section = {"id": s_id, "title": content_title}

            elif marker_type.startswith("ƒêi·ªÅu"):
                end_idx = (
                    matches[i + 1].start() if i + 1 < len(matches) else len(full_text)
                )

                # Offset b·∫Øt ƒë·∫ßu c·ªßa ƒêi·ªÅu trong to√†n b·ªô vƒÉn b·∫£n
                article_global_start = match.start()

                raw_article_text = full_text[article_global_start:end_idx].strip()
                art_id, full_art_title, content_body, body_rel_offset = (
                    self._extract_article_info(raw_article_text)
                )

                if not art_id:
                    continue  # B·ªè qua n·∫øu kh√¥ng t√¨m th·∫•y ID

                # Metadata cho ƒêi·ªÅu
                meta = {
                    "law_id": self.law_id,
                    "chapter_id": self.current_chapter["id"],
                    "chapter_title": self.current_chapter["title"],
                    "section_id": self.current_section["id"],
                    "section_title": self.current_section["title"],
                    "article_id": art_id,
                    "article_title": full_art_title,
                    "topic": "legal_document",  # Placeholder
                    "source_file": self.pdf_path.split("/")[-1],
                    "footnotes": "",  # Placeholder cho t∆∞∆°ng lai
                }

                # Tinh ch·ªânh offset truy·ªÅn v√†o recursive_split
                # recursive_split x·ª≠ l√Ω tr√™n content_body, n√™n base_offset ph·∫£i c·ªông th√™m ph·∫ßn ti√™u ƒë·ªÅ ƒë√£ c·∫Øt
                final_body_offset = article_global_start + body_rel_offset

                # G·ªçi h√†m c·∫Øt
                chunks = self.recursive_split(
                    {
                        "id": art_id,
                        "title": full_art_title,
                        "content": content_body,
                        "metadata": meta,
                    },
                    base_offset=final_body_offset,  # [FIX] Truy·ªÅn offset ch√≠nh x√°c
                )
                self.chunks.extend(chunks)
                print(
                    f"   ‚úì ƒêi·ªÅu {art_id}: {len(chunks)} chunks | T·ªïng: {len(self.chunks)}"
                )

            # Checkpoint every 50 items
            if i > 0 and (i + 1) % 50 == 0:
                print(
                    f"\nüéØ Checkpoint: {i+1}/{len(matches)} ({(i+1)/len(matches)*100:.1f}%) - {len(self.chunks)} chunks t·ªïng\n"
                )

        print(f"\n‚úÖ Ho√†n th√†nh! T·ªïng c·ªông {len(self.chunks)} chunks ƒë∆∞·ª£c t·∫°o ra.")
        return self.chunks


# ==========================================
# TEST RUNNER (ƒê·ªÉ b·∫°n ch·∫°y th·ª≠)
# ==========================================
if __name__ == "__main__":
    # Thay t√™n file PDF c·ªßa b·∫°n v√†o ƒë√¢y
    PDF_FILE = "133-vbhn-vpqh.pdf"

    try:
        parser = LandLawChunkerFinal(PDF_FILE)
        final_data = parser.process()

        # Page 218 is image, so fitz can not load content, temp handle this way
        # Remove the last item from final_data
        if final_data:
            final_data.pop()
            print(f"üóëÔ∏è ƒê√£ x√≥a item cu·ªëi c√πng. C√≤n l·∫°i: {len(final_data)} chunks")

        # Load and concatenate content from law_content_page_128.json
        EXTERNAL_JSON_FILE = "law_content_page_128.json"
        try:
            with open(EXTERNAL_JSON_FILE, "r", encoding="utf-8") as f:
                external_data = json.load(f)

            if isinstance(external_data, list):
                final_data.extend(external_data)
                print(f"‚ûï ƒê√£ th√™m {len(external_data)} chunks t·ª´ {EXTERNAL_JSON_FILE}")
                print(f"üìä T·ªïng c·ªông sau khi gh√©p: {len(final_data)} chunks")
            else:
                print(
                    f"‚ö†Ô∏è C·∫£nh b√°o: {EXTERNAL_JSON_FILE} kh√¥ng ph·∫£i l√† array, b·ªè qua vi·ªác gh√©p"
                )

        except FileNotFoundError:
            print(
                f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file {EXTERNAL_JSON_FILE}, ti·∫øp t·ª•c v·ªõi d·ªØ li·ªáu hi·ªán t·∫°i"
            )
        except json.JSONDecodeError as e:
            print(f"‚ùå L·ªói ƒë·ªçc JSON t·ª´ {EXTERNAL_JSON_FILE}: {e}")

        # Xu·∫•t k·∫øt qu·∫£
        OUTPUT_FILE = "land_law_chunks_final.json"
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

        print(f"üíæ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {OUTPUT_FILE}")

    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
